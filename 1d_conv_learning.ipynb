{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cb1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78582b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_file(df):\n",
    "    df = df[0].str.split('\\t', n=4, expand=True)\n",
    "\n",
    "    df.columns = [\"time\", \"sensor\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "    GYR_data = df.loc[df['sensor'] == 'GYR']\n",
    "    ACC_data = df.loc[df['sensor'] == 'ACC']\n",
    "\n",
    "    GYR_data.index = GYR_data['time'].apply(pd.to_numeric)\n",
    "    ACC_data.index = ACC_data['time'].apply(pd.to_numeric)\n",
    "    GYR_data = GYR_data.drop('sensor', axis=1)\n",
    "    ACC_data = ACC_data.drop('sensor', axis=1)\n",
    "    GYR_data.columns = ['time_gyr', \"gyr_x\", \"gyr_y\", \"gyr_z\"]\n",
    "    ACC_data.columns = ['time_acc', \"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "    \n",
    "    GYR_data = GYR_data.apply(pd.to_numeric)\n",
    "    ACC_data = ACC_data.apply(pd.to_numeric)\n",
    "    \n",
    "    min_idx = np.argmin([ACC_data.shape[0], GYR_data.shape[0]]) \n",
    "    max_idx = np.argmax([ACC_data.shape[0], GYR_data.shape[0]])  \n",
    "\n",
    "    data_acc_gyr = [ACC_data, GYR_data] \n",
    "    data_acc_gyr[max_idx] = data_acc_gyr[max_idx].reindex(data_acc_gyr[min_idx].index, method='nearest', tolerance=0.02)\n",
    "    new_set = pd.concat(data_acc_gyr, axis=1)\n",
    "    new_set2 = new_set.dropna().reset_index(drop=True)\n",
    "    return new_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2224fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_running_oscar = structure_file(pd.read_csv('running/running_oscar.txt', header=None))\n",
    "data_walking_walking = structure_file(pd.read_csv('walking/walking_oscar.txt', header=None))\n",
    "data_standing_oscar = structure_file(pd.read_csv('standing_still/standing_still_oscar.txt', header=None))\n",
    "\n",
    "data_running_oscar = structure_file(pd.read_csv('running/running_oscar.txt', header=None))\n",
    "data_walking_walking = structure_file(pd.read_csv('walking/walking_oscar.txt', header=None))\n",
    "data_standing_oscar = structure_file(pd.read_csv('standing_still/standing_still_oscar.txt', header=None))\n",
    "\n",
    "all_data = {'running':[],\n",
    "           'walking':[],\n",
    "           'standing_still':[]}\n",
    "\n",
    "\n",
    "data_types = [\"standing_still\", \"walking\", \"running\"]\n",
    "current_path = os.path.abspath('.')\n",
    "for data_type in data_types:\n",
    "    data_path = os.path.join(current_path, data_type)\n",
    "    for file in os.listdir(data_path):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        data = pd.read_csv(file_path, header=None)\n",
    "        pandas_data = structure_file(data)\n",
    "        all_data[data_type].append(pandas_data)\n",
    "        # pprint(pandas_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01a28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create sequence of data\n",
    "num_channels = 6\n",
    "time_series_len = 200\n",
    "\n",
    "num_samples = 0\n",
    "for key in all_data:\n",
    "    for data_sample in all_data[key]:\n",
    "        nr_sample = 0\n",
    "        while nr_sample < data_sample.shape[0]//time_series_len -1:\n",
    "            nr_sample += 1\n",
    "            num_samples += 1            \n",
    "\n",
    "#data_np = np.zeros((num_samples, num_channels, time_series_len))\n",
    "#label_np = np.zeros(num_samples)\n",
    "\n",
    "\n",
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "\n",
    "key_to_label = {'running': np.array([1, 0, 0]),\n",
    "                'walking':  np.array([0, 1, 0]),\n",
    "                'standing_still':  np.array([0, 0, 1])}\n",
    "\n",
    "channels = ['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z']\n",
    "\n",
    "nr_sample = 0\n",
    "for key in all_data:\n",
    "    nr_sub_sample = 0\n",
    "    for data_sample in all_data[key]:\n",
    "        start_v = 0\n",
    "        while nr_sub_sample < data_sample.shape[0]//time_series_len - 1:\n",
    "            label_list.append(key_to_label[key])\n",
    "            data_sample_np = np.zeros((num_channels, time_series_len))\n",
    "            for i in range(num_channels):\n",
    "                data_key = channels[i]\n",
    "                data_sample_np[i, :] = data_sample[data_key].iloc[start_v: start_v+time_series_len]\n",
    "            data_list.append(data_sample_np.T)\n",
    "            start_v += time_series_len\n",
    "            nr_sub_sample += 1\n",
    "            nr_sample += 1\n",
    "            \n",
    "\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(data_list, label_list, test_size=0.4, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test_val, y_test_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff599792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45930/2617451969.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor_x_train = torch.Tensor(x_train)\n"
     ]
    }
   ],
   "source": [
    "tensor_x_train = torch.Tensor(x_train)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "\n",
    "tensor_x_test = torch.Tensor(x_test)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "tensor_x_val = torch.Tensor(x_val)\n",
    "tensor_y_val = torch.Tensor(y_val)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train) \n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test) \n",
    "val_dataset = TensorDataset(tensor_x_val, tensor_y_val) \n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d17e",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddfe8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Model, self).__init__()\n",
    "        self.kernel_size = 21\n",
    "\n",
    "        # conv layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=6, \n",
    "                               out_channels=12, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=1,\n",
    "                               padding=self._padding(1),\n",
    "                               bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=12, \n",
    "                               out_channels=24, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=1,\n",
    "                               padding=self._padding(1),\n",
    "                               bias=False)\n",
    "        self.conv3 = nn.Conv1d(in_channels=24, \n",
    "                               out_channels=48, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=2,\n",
    "                               padding=self._padding(2),\n",
    "                               bias=False)\n",
    "        self.conv4 = nn.Conv1d(in_channels=48, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=2,\n",
    "                               padding=self._padding(2),\n",
    "                               bias=False)\n",
    "        self.conv5 = nn.Conv1d(in_channels=96, \n",
    "                               out_channels=192, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=2,\n",
    "                               padding=self._padding(2),\n",
    "                               bias=False)\n",
    "        self.conv6 = nn.Conv1d(in_channels=192, \n",
    "                               out_channels=192, \n",
    "                               kernel_size=self.kernel_size, \n",
    "                               stride=2,\n",
    "                               padding=self._padding(2),\n",
    "                               bias=False)                                                         \n",
    "        # linear layer\n",
    "        self.lin1 = nn.LazyLinear(out_features=1024)     \n",
    "        self.lin2 = nn.LazyLinear(out_features=256)\n",
    "        self.lin3 = nn.LazyLinear(out_features=3)        \n",
    "\n",
    "        # ReLU\n",
    "        self.bn1 = nn.BatchNorm1d(12)\n",
    "        self.bn2 = nn.BatchNorm1d(24)\n",
    "        self.bn3 = nn.BatchNorm1d(48)\n",
    "        self.bn4 = nn.BatchNorm1d(96)\n",
    "        self.bn5 = nn.BatchNorm1d(192)        \n",
    "        self.bn6 = nn.BatchNorm1d(192)   \n",
    "        self.relu = nn.ReLU()\n",
    "        self.d_small=nn.Dropout(p=0.25)\n",
    "        self.d=nn.Dropout(p=0.5)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm1d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _padding(self, downsample):\n",
    "        return max(0, int(np.floor((self.kernel_size - downsample + 1) / 2)))\n",
    "\n",
    "    def _downsample(self, seq_len_in, seq_len_out):\n",
    "        return int(seq_len_in // seq_len_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(2,1)\n",
    "\n",
    "        x = self.bn1(self.d_small(self.relu(self.conv1(x))))\n",
    "        x = self.bn2(self.d_small(self.relu(self.conv2(x))))\n",
    "        x = self.bn3(self.d_small(self.relu(self.conv3(x))))\n",
    "        x = self.bn4(self.d_small(self.relu(self.conv4(x))))\n",
    "        x = self.bn5(self.d_small(self.relu(self.conv5(x))))    \n",
    "        x = self.bn6(self.d_small(self.relu(self.conv6(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.d(self.relu(self.lin1(x)))\n",
    "        x = self.d(self.relu(self.lin2(x)))\n",
    "        x = F.log_softmax(self.lin3(x), dim=1)\n",
    "        return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8646d1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabwal/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.to(device);\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691ab068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_data_loader, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            pred = model(b_x.to(device))\n",
    "            loss = loss_fn(pred, b_y.to(device))\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y.argmax(dim=1).to(device)).item()\n",
    "        val_accuracy = n_correct/len(val_data_loader.dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ff51ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \tLoss: 0.4567 \tLoss (val): 0.3315\tAccuracy: 0.7209 \tAccuracy (val): 0.9087\n",
      "Epoch 1 \tLoss: 0.2956 \tLoss (val): 0.2670\tAccuracy: 0.9418 \tAccuracy (val): 0.9762\n",
      "Epoch 2 \tLoss: 0.2563 \tLoss (val): 0.2703\tAccuracy: 0.9828 \tAccuracy (val): 0.9802\n",
      "Epoch 3 \tLoss: 0.2514 \tLoss (val): 0.2682\tAccuracy: 0.9868 \tAccuracy (val): 0.9722\n",
      "Epoch 4 \tLoss: 0.2447 \tLoss (val): 0.2439\tAccuracy: 0.9894 \tAccuracy (val): 0.9881\n",
      "Epoch 5 \tLoss: 0.2434 \tLoss (val): 0.2343\tAccuracy: 0.9921 \tAccuracy (val): 0.9960\n",
      "Epoch 6 \tLoss: 0.2361 \tLoss (val): 0.2343\tAccuracy: 0.9960 \tAccuracy (val): 0.9960\n",
      "Epoch 7 \tLoss: 0.2374 \tLoss (val): 0.2569\tAccuracy: 0.9960 \tAccuracy (val): 0.9762\n",
      "Epoch 8 \tLoss: 0.2447 \tLoss (val): 0.2444\tAccuracy: 0.9921 \tAccuracy (val): 0.9881\n",
      "Epoch 9 \tLoss: 0.2409 \tLoss (val): 0.2330\tAccuracy: 0.9947 \tAccuracy (val): 0.9960\n",
      "Epoch 10 \tLoss: 0.2373 \tLoss (val): 0.2349\tAccuracy: 0.9907 \tAccuracy (val): 0.9921\n",
      "Epoch 11 \tLoss: 0.2324 \tLoss (val): 0.2473\tAccuracy: 0.9987 \tAccuracy (val): 0.9921\n",
      "Epoch 12 \tLoss: 0.2410 \tLoss (val): 0.2636\tAccuracy: 0.9947 \tAccuracy (val): 0.9802\n",
      "Epoch 13 \tLoss: 0.2348 \tLoss (val): 0.2349\tAccuracy: 0.9987 \tAccuracy (val): 0.9960\n",
      "Epoch 14 \tLoss: 0.2343 \tLoss (val): 0.2345\tAccuracy: 0.9974 \tAccuracy (val): 0.9960\n"
     ]
    }
   ],
   "source": [
    "def train_network(model, loss_fn, optimizer, num_epochs, train_data_loader, val_data_loader):\n",
    "\n",
    "    val_loss_data = []\n",
    "    train_loss_data = []\n",
    "\n",
    "    val_acc_data = []\n",
    "    train_acc_data = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        for b_x, b_y in train_data_loader:\n",
    "\n",
    "            # Compute predictions and losses\n",
    "            pred = model(b_x.to(device))\n",
    "            loss = loss_fn(pred, b_y.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            #print(pred.argmax(dim=1))\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y.argmax(dim=1).to(device)).item()\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()    \n",
    "\n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_accuracy = n_correct/len(train_data_loader.dataset)\n",
    "        train_avg_loss = sum(losses)/len(losses)    \n",
    "\n",
    "        # Compute accuracy and loss in the entire validation set\n",
    "        val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "\n",
    "        # Display metrics\n",
    "        display_str = 'Epoch {} '\n",
    "        display_str += '\\tLoss: {:.4f} '\n",
    "        display_str += '\\tLoss (val): {:.4f}'\n",
    "        display_str += '\\tAccuracy: {:.4f} '\n",
    "        display_str += '\\tAccuracy (val): {:.4f}'\n",
    "        print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "        val_loss_data.append(val_avg_loss)\n",
    "        train_loss_data.append(train_avg_loss)\n",
    "\n",
    "        val_acc_data.append(val_accuracy)\n",
    "        train_acc_data.append(train_accuracy)\n",
    "\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return val_loss_data, train_loss_data, val_acc_data, train_acc_data\n",
    "\n",
    "val_loss_data, train_loss_data, val_acc_data, train_acc_data = train_network(model=model, \n",
    "                                                                             loss_fn=loss_fn,\n",
    "                                                                             optimizer=optimizer, \n",
    "                                                                             num_epochs=15,\n",
    "                                                                             train_data_loader=train_data_loader,\n",
    "                                                                             val_data_loader=test_data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2478ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy 0.9920634920634921\n"
     ]
    }
   ],
   "source": [
    "val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "print('val_accuracy',val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0ca84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdb66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
