{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41bb0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc869169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/Work/Studies/WASP/AutonomousSystems/activitty_recognition/venv/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(6, 12, kernel_size=(3,), stride=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv1d(12, 24, kernel_size=(3,), stride=(1,))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv1d(24, 48, kernel_size=(3,), stride=(1,))\n",
       "  (7): ReLU()\n",
       "  (8): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       "  (10): LazyLinear(in_features=0, out_features=48, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): Linear(in_features=24, out_features=12, bias=True)\n",
       "  (15): ReLU()\n",
       "  (16): Linear(in_features=12, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input shape = B, 6 (or however many channels we want), len\n",
    "\n",
    "# Conv1d(in_channels, out_channels, \n",
    "# kernel_size, stride=1, padding=0, dilation=1, \n",
    "# groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "\n",
    "\n",
    "c_in = 6\n",
    "num_classes = 3\n",
    "\n",
    "channels = [12, 24, 48]\n",
    "strides = [1, 1, 1]\n",
    "paddings = [0, 0, 0]\n",
    "dilations = [1, 1, 1]\n",
    "ksizes = [3, 3, 3]\n",
    "use_pool = True\n",
    "\n",
    "pool_ksize = 2\n",
    "pool_stride = 1\n",
    "pool_dilation = 1\n",
    "pool_padding = 0\n",
    "\n",
    "linear_features = [48, 24, 12]\n",
    "\n",
    "# torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "\n",
    "model = nn.Sequential()\n",
    "\n",
    "for layer_ind, (out_channels, ksize, stride, padding, dilation) in enumerate(zip(channels, ksizes, strides, paddings, dilations)):\n",
    "    if layer_ind == 0:\n",
    "        in_channels = c_in\n",
    "    else:\n",
    "        in_channels = channels[layer_ind - 1]\n",
    "        \n",
    "    model.append(\n",
    "        nn.Conv1d(in_channels=in_channels, kernel_size=ksize, out_channels=out_channels, stride=stride, padding=padding, dilation=dilation)\n",
    "    )\n",
    "    model.append(\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    if use_pool:\n",
    "        model.append(\n",
    "            nn.MaxPool1d(kernel_size = pool_ksize, stride=pool_stride, padding=pool_padding, dilation=pool_dilation)\n",
    "        )\n",
    "        \n",
    "model.append(\n",
    "    nn.Flatten()\n",
    ")\n",
    "        \n",
    "for lin_ind, lin_features in enumerate(linear_features):\n",
    "    \n",
    "    out_features = lin_features\n",
    "    if lin_ind == 0:\n",
    "        model.append(\n",
    "            nn.LazyLinear(out_features)\n",
    "        )\n",
    "    else:\n",
    "        in_features = linear_features[lin_ind - 1]\n",
    "        \n",
    "        model.append(\n",
    "            nn.Linear(in_features = in_features, out_features = out_features)\n",
    "        )\n",
    "    model.append(\n",
    "        nn.ReLU()\n",
    "    )\n",
    "        \n",
    "model.append(\n",
    "    nn.Linear(in_features = linear_features[-1], out_features = num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82902ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x47568 and 4368x48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model(test_data)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m1000\u001b[39m)),dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Work/Studies/WASP/AutonomousSystems/activitty_recognition/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Work/Studies/WASP/AutonomousSystems/activitty_recognition/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Work/Studies/WASP/AutonomousSystems/activitty_recognition/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Work/Studies/WASP/AutonomousSystems/activitty_recognition/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x47568 and 4368x48)"
     ]
    }
   ],
   "source": [
    "test_data = torch.tensor(np.random.random((10,6,100)),dtype=torch.float32)\n",
    "model(test_data).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wasp_as_venv",
   "language": "python",
   "name": "wasp_as_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
